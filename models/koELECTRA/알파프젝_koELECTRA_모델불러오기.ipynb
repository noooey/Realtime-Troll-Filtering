{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/KoELECTRA/finetune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFByhyDWbpL-",
        "outputId": "f34310dc-c942-4abf-db57-75f40db1eb83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/KoELECTRA/finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZQYQSaaEifn",
        "outputId": "b18981cd-7d38-47fb-c621-69836e68e408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=9c2b8e6523d15018ef25c15840270970bec2c1802f11656e824275543abfc592\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from src import (\n",
        "    CONFIG_CLASSES,\n",
        "    TOKENIZER_CLASSES,\n",
        "    MODEL_FOR_SEQUENCE_CLASSIFICATION,\n",
        "    init_logger,\n",
        "    set_seed,\n",
        "    compute_metrics\n",
        ")\n",
        "from processor import seq_cls_load_and_cache_examples as load_and_cache_examples\n",
        "from processor import seq_cls_tasks_num_labels as tasks_num_labels\n",
        "from processor import seq_cls_processors as processors\n",
        "from processor import seq_cls_output_modes as output_modes\n",
        "\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "25vY5RPtbjw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"Haaaaeun/koELECTRA_hatespeech\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer_v1 = AutoTokenizer.from_pretrained(model_name)\n",
        "model_v1 = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "8xq1ie5bc-Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Haaaaeun/koELECTRA_hatespeech_v2\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer_v2 = AutoTokenizer.from_pretrained(model_name)\n",
        "model_v2 = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "k_K5wUDPhVvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the device (CPU or GPU) for inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_v1.to(device)\n",
        "model_v2.to(device)\n"
      ],
      "metadata": {
        "id": "sd-BSScQlOv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example input text\n",
        "input_text = \"딱 봐도 모르냐 ㅂㅅ.\"\n"
      ],
      "metadata": {
        "id": "XRflz_rZlaAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input text\n",
        "inputs_v1 = tokenizer_v1.encode_plus(input_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs_v1 = inputs_v1.to(device)\n",
        "\n",
        "# Perform inference\n",
        "model_v1.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_v1 = model_v1(**inputs_v1)\n",
        "\n",
        "# Process the outputs\n",
        "logits_v1 = outputs_v1.logits\n",
        "probabilities_v1 = torch.softmax(logits_v1, dim=1)\n",
        "prediction_v1 = torch.argmax(probabilities_v1, dim=1).item()\n",
        "\n",
        "# Print the prediction\n",
        "print(\"Prediction:\", prediction_v1)\n"
      ],
      "metadata": {
        "id": "G9tg0_rNgbnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input text\n",
        "inputs_v2 = tokenizer_v2.encode_plus(input_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs_v2 = inputs_v2.to(device)\n",
        "\n",
        "# Perform inference\n",
        "model_v2.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_v2 = model_v2(**inputs_v2)\n",
        "\n",
        "# Process the outputs\n",
        "logits_v2 = outputs_v2.logits\n",
        "probabilities_v2 = torch.softmax(logits_v2, dim=1)\n",
        "prediction_v2 = torch.argmax(probabilities_v2, dim=1).item()\n",
        "\n",
        "# Print the prediction\n",
        "print(\"Prediction:\", prediction_v2)\n"
      ],
      "metadata": {
        "id": "u7lNPw-nk-9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s0GpFP81lBFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}